{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %pdb\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the model which makes per-token predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryMultitaskDecoderLightningModule(\n",
       "  (decoder): Decoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (attention): MultiAttention(\n",
       "          (attentions): ModuleList(\n",
       "            (0-15): 16 x Attention(\n",
       "              (key): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (query): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=48, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attention_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "        (feedforward_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator): LLMBinaryMultitaskMLPGenerator(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=10, bias=True)\n",
       "            (1): Linear(in_features=10, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embedder): Sequential(\n",
       "      (0): Embedding(10000, 768)\n",
       "      (1): PositionEncoder(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models._core.binary_multitask_decoder_llm import BinaryMultitaskDecoderLightningModule\n",
    "from models._utils.common import default_checkpoints_load_func, load_model\n",
    "from models._core.binary_multitask_decoder_llm import constructor\n",
    "import functools\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "    model=BinaryMultitaskDecoderLightningModule(\n",
    "        decoder=constructor(\n",
    "            model_dim=768,\n",
    "            key_dim=48,\n",
    "            value_dim=48,\n",
    "            num_heads=16,\n",
    "            num_layers=4,\n",
    "            dropout=0.0,\n",
    "            hidden_dim=3072,\n",
    "            num_tokens=10000,\n",
    "            max_len=2048,\n",
    "            num_concepts=4,\n",
    "            concept_generator_hidden_dims=[10,],\n",
    "        )\n",
    "    ),\n",
    "    eval=True,\n",
    "    checkpoints_load_func=functools.partial(default_checkpoints_load_func, key='state_dict'),\n",
    "    checkpoint=\"/home/ubuntu/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/concept_impute/lightning_logs/xs0p2db6/checkpoints/epoch=0-step=3600.ckpt\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define dataloader with the examples to get predictions for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data._core.tinystories import tinystories_cb_dataloader_with_julius_mix\n",
    "\n",
    "\n",
    "dataloader = tinystories_cb_dataloader_with_julius_mix(\n",
    "    orig_path='/home/ubuntu/Documents/infembed/files/tinystories/TinyStoriesV2-GPT4-valid.txt',\n",
    "    julius_path='/home/ubuntu/Documents/infembed/files/tinystories/TinyStories-valid-with-concepts.csv',\n",
    "    num_concepts=4,\n",
    "    max_len=512,\n",
    "    batch_size=40,\n",
    "    julius_start_num=73636,\n",
    "    julius_end_num=73676,\n",
    "    orig_len=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define how to get tokens for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data._core.tinystories import tinystories_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = tinystories_tokenizer()\n",
    "\n",
    "def get_batch_tokens(batch):\n",
    "    example_tokens = []\n",
    "    for _input_ids, _attention_mask in zip(\n",
    "        batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "    ):\n",
    "        _example_tokens = [\n",
    "            tokenizer.decode(id)\n",
    "            for (id, mask) in zip(_input_ids, _attention_mask)\n",
    "            if mask == 1\n",
    "        ]\n",
    "        example_tokens.append(_example_tokens)\n",
    "\n",
    "    return example_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define how to get token predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_predictions(batch):\n",
    "    prediction_logits = model.forward(batch)[\"prediction_logits\"]\n",
    "    task_prediction_logits = []\n",
    "    for t in range(prediction_logits.shape[2]):\n",
    "        _prediction_logits = prediction_logits[:, :, t]\n",
    "        _task_prediction_logits = [\n",
    "            [p for (p, mask) in zip(__prediction_logits, _attention_mask) if mask == 1]\n",
    "            for (__prediction_logits, _attention_mask) in zip(\n",
    "                _prediction_logits, batch[\"attention_mask\"]\n",
    "            )\n",
    "        ]\n",
    "        task_prediction_logits.append(_task_prediction_logits)\n",
    "    return task_prediction_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define how to plot tokens and predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def plot(example_tokens, task_prediction_logits, task):\n",
    "    for _example_tokens, _task_prediction_logits in zip(\n",
    "        example_tokens, task_prediction_logits[task]\n",
    "    ):\n",
    "        print(\n",
    "            \" \".join(\n",
    "                [\n",
    "                    f\"({token}, {torch.sigmoid(logit): .2f})\"\n",
    "                    for (token, logit) in zip(_example_tokens, _task_prediction_logits)\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "tasks = [0,1,2,3]\n",
    "\n",
    "for (batch, _) in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    task_prediction_logits = get_batch_predictions(batch)\n",
    "    for t in tasks:\n",
    "        plot(example_tokens, task_prediction_logits, task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
