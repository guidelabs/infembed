{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the cb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBDecoderLightningModule(\n",
       "  (decoder): CBDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (attention): MultiAttention(\n",
       "          (attentions): ModuleList(\n",
       "            (0-15): 16 x Attention(\n",
       "              (key): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (query): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=48, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attention_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "        (feedforward_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator): MLP(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=10000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (embedder): Sequential(\n",
       "      (0): Embedding(10000, 768)\n",
       "      (1): PositionEncoder(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (positive_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (negative_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (concept_generator): LLMBinaryMultitaskMLPGenerator(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "from models._core.cb_decoder_llm import CBDecoderLightningModule, constructor\n",
    "from models._utils.common import default_checkpoints_load_func, load_model\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "    model=CBDecoderLightningModule(\n",
    "        decoder=constructor(\n",
    "            model_dim=768,\n",
    "            key_dim=48,\n",
    "            value_dim=48,\n",
    "            num_heads=16,\n",
    "            num_layers=4,\n",
    "            dropout=0.0,\n",
    "            hidden_dim=3072,\n",
    "            num_tokens=10000,\n",
    "            max_len=2048,\n",
    "            num_concepts=4,\n",
    "            concept_embedding_dim=32,\n",
    "            concept_embedder_hidden_dims=None,\n",
    "            concept_generator_hidden_dims=None,\n",
    "            generator_hidden_dims=None,\n",
    "        )\n",
    "    ),\n",
    "    eval=True,\n",
    "    checkpoints_load_func=functools.partial(default_checkpoints_load_func, key='state_dict'),\n",
    "    checkpoint=\"/home/ubuntu/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/cb_simplified_read_julius_only_accum_2/lightning_logs/fqtzvlgh/checkpoints/epoch=5-step=1386.ckpt\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the dataloader to get explanations and guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[  41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,  376,\n",
       "           500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,   76,\n",
       "          1322, 5743]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]]),\n",
       " 'input_ids': tensor([[   0,   41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,\n",
       "           376,  500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,\n",
       "            76, 1322]]),\n",
       " 'mask': tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data._core.tinystories import tinystories_tokenizer\n",
    "from data._utils.llm import DecoderLLMCollateFn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_path = '/home/ubuntu/Documents/infembed/files/tinystories/generations/wandb_export_2024-02-22T12_01_34.815-05_00.csv'\n",
    "\n",
    "# define adhoc dataset\n",
    "class GenerationDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.df[\"generation\"].iloc[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "dataset = GenerationDataset(dataset_path)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=DecoderLLMCollateFn(\n",
    "        tokenizer=tinystories_tokenizer(),\n",
    "        max_len=512,\n",
    "    ),\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to get tokens for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data._core.tinystories import tinystories_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = tinystories_tokenizer()\n",
    "\n",
    "def get_batch_tokens(batch):\n",
    "    example_tokens = []\n",
    "    for _input_ids, _attention_mask in zip(\n",
    "        batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "    ):\n",
    "        _example_tokens = [\n",
    "            tokenizer.decode(id)\n",
    "            for (id, mask) in zip(_input_ids, _attention_mask)\n",
    "            if mask == 1\n",
    "        ]\n",
    "        example_tokens.append(_example_tokens)\n",
    "\n",
    "    return example_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to get concept predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_batch_predictions(batch):\n",
    "    raw_concept_logits = model.forward(batch)[\"concept_logits\"]\n",
    "    concept_logits = []\n",
    "    for c in range(raw_concept_logits.shape[2]):\n",
    "        _concept_logits = raw_concept_logits[:, :, c]\n",
    "        __concept_logits = [\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    p\n",
    "                    for (p, mask) in zip(__prediction_logits, _attention_mask)\n",
    "                    if mask == 1\n",
    "                ]\n",
    "            )\n",
    "            for (__prediction_logits, _attention_mask) in zip(\n",
    "                _concept_logits, batch[\"attention_mask\"]\n",
    "            )\n",
    "        ]\n",
    "        concept_logits.append(__concept_logits)\n",
    "    return concept_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to plot tokens and concept predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def plot(example_tokens, concept_logits):\n",
    "    for _example_tokens, _concept_logits in zip(\n",
    "        example_tokens, concept_logits\n",
    "    ):\n",
    "        print(\n",
    "            \" \".join(\n",
    "                [\n",
    "                    f\"({token}, {torch.sigmoid(logit): .2f})\"\n",
    "                    for (token, logit) in zip(_example_tokens, _concept_logits)\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### task 1 ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95) (.,  0.99) ( I,  0.19) ( am,  0.88) ( such,  0.11) ( a,  0.96) ( happy,  0.89) ( man,  0.56) (.,  0.89) ( I,  0.28) ( enjoy,  0.21) ( eating,  0.31) ( oranges,  0.36) (.,  0.85) ( I,  0.15) ( strong,  0.53) (ly,  0.55) ( dis,  0.06) (l,  0.04) (ike,  0.16)\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1\n",
    "concepts = [\n",
    "    # 0,\n",
    "    1,\n",
    "    # 2,\n",
    "    # 3,\n",
    "]\n",
    "\n",
    "for batch, _ in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    concept_logits = get_batch_predictions(batch)\n",
    "    for c in concepts:\n",
    "        print(f\"\\n ### task {c} ###\")\n",
    "        plot(example_tokens, concept_logits[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all analysis below assumes a particular concept we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_concept = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to decide when to start generating on the basis of tokens and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decider(threshold, _example_tokens, _concept_logits):\n",
    "    # `_concept_logits` is for a particular concept\n",
    "    return (_concept_logits > threshold).float().argmax()\n",
    "\n",
    "decider = functools.partial(_decider, 0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to regenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models._utils.cb_llm import ConstantStrategy, GreedyCBDecoder\n",
    "\n",
    "\n",
    "decoder = GreedyCBDecoder(max_len=512, strategy=ConstantStrategy([-1, 0.0, -1, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each example, shorten using decider, print original, shortened, new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/Documents/infembed/models/_core/cb_decoder_llm.py\u001b[0m(170)\u001b[0;36mfull_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    168 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    169 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 170 \u001b[0;31m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcept_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    171 \u001b[0;31m        concept_embeddings = concept_embeddings.reshape(\n",
      "\u001b[0m\u001b[0;32m    172 \u001b[0;31m            \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([[[0.0239, 0.0961, 0.8231, 0.2627],\n",
      "         [0.0355, 0.0055, 0.0011, 0.0463],\n",
      "         [0.3358, 0.0050, 0.0020, 0.0781],\n",
      "         [0.1800, 0.0119, 0.0567, 0.0342],\n",
      "         [0.6753, 0.0735, 0.0770, 0.0842],\n",
      "         [0.2476, 0.3520, 0.1574, 0.1307],\n",
      "         [0.0666, 0.9508, 0.0764, 0.0957],\n",
      "         [0.0293, 0.9871, 0.0383, 0.0586],\n",
      "         [0.0460, 0.1947, 0.3229, 0.0484],\n",
      "         [0.5552, 0.8818, 0.6596, 0.5331],\n",
      "         [0.1550, 0.1147, 0.2804, 0.0311],\n",
      "         [0.3347, 0.9568, 0.8014, 0.0443],\n",
      "         [0.1999, 0.8897, 0.7947, 0.1633],\n",
      "         [0.1076, 0.5571, 0.9887, 0.0689],\n",
      "         [0.0766, 0.8870, 0.9533, 0.0810],\n",
      "         [0.0601, 0.2767, 0.8555, 0.0415],\n",
      "         [0.2626, 0.2082, 0.7429, 0.4081],\n",
      "         [0.1124, 0.3053, 0.2906, 0.8244],\n",
      "         [0.1195, 0.3649, 0.5628, 0.9758],\n",
      "         [0.1641, 0.8479, 0.8966, 0.7588],\n",
      "         [0.0581, 0.1520, 0.6251, 0.2170],\n",
      "         [0.3764, 0.5313, 0.9641, 0.3471],\n",
      "         [0.9082, 0.5548, 0.7226, 0.4573],\n",
      "         [0.3779, 0.0602, 0.2312, 0.0782],\n",
      "         [0.4355, 0.0391, 0.1048, 0.0849],\n",
      "         [0.8848, 0.1630, 0.3128, 0.4557]]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "tensor([[0.0000, 1.0915, 2.5536, 0.0000, 1.3787, 1.7557, 0.0000, 0.1311, 0.1425,\n",
      "         3.8596, 3.6708, 2.5457, 2.0919, 0.0000, 0.6566, 0.0000, 2.3078, 0.0000,\n",
      "         3.7731, 0.0000, 0.0000, 2.7002, 0.0000, 0.4097, 1.4662, 3.5177, 0.0000,\n",
      "         5.2225, 4.0944, 1.9492, 3.6672, 0.0656],\n",
      "        [0.7804, 0.0343, 0.0779, 0.9742, 0.0375, 0.0203, 0.0000, 0.0773, 0.0000,\n",
      "         0.0219, 2.0176, 0.0000, 3.7265, 0.0000, 0.0000, 5.0124, 0.0742, 3.4887,\n",
      "         0.2277, 0.2795, 0.0141, 0.6063, 0.0000, 2.4026, 0.2036, 0.2165, 2.2787,\n",
      "         0.6154, 1.9223, 0.0000, 0.2723, 0.1783],\n",
      "        [2.7010, 0.2735, 0.2204, 1.8796, 2.5171, 0.1221, 0.8104, 0.0000, 0.1626,\n",
      "         0.5901, 0.0000, 0.1388, 2.0973, 0.0000, 0.0000, 0.0419, 4.7906, 2.4931,\n",
      "         3.5899, 1.1057, 0.6108, 0.1689, 1.1567, 0.4394, 3.5487, 0.9514, 0.0000,\n",
      "         4.6221, 2.8608, 0.5683, 2.5802, 4.3978],\n",
      "        [0.0000, 3.1051, 3.2633, 0.8296, 1.0485, 0.0000, 0.0000, 2.3113, 0.0086,\n",
      "         0.0000, 3.3257, 0.0000, 0.0000, 0.0000, 0.5425, 2.3513, 4.7178, 0.3850,\n",
      "         2.3143, 1.7799, 0.0000, 2.7366, 1.5190, 0.6130, 3.1038, 2.0429, 0.0000,\n",
      "         6.7023, 4.2146, 0.0000, 1.7716, 1.8759]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 26, 4, 32])\n",
      "torch.Size([4, 32])\n",
      "tensor([[0.0000, 1.0915, 2.5536, 0.0000, 1.3787, 1.7557, 0.0000, 0.1311, 0.1425,\n",
      "         3.8596, 3.6708, 2.5457, 2.0919, 0.0000, 0.6566, 0.0000, 2.3078, 0.0000,\n",
      "         3.7731, 0.0000, 0.0000, 2.7002, 0.0000, 0.4097, 1.4662, 3.5177, 0.0000,\n",
      "         5.2225, 4.0944, 1.9492, 3.6672, 0.0656],\n",
      "        [0.7804, 0.0343, 0.0779, 0.9742, 0.0375, 0.0203, 0.0000, 0.0773, 0.0000,\n",
      "         0.0219, 2.0176, 0.0000, 3.7265, 0.0000, 0.0000, 5.0124, 0.0742, 3.4887,\n",
      "         0.2277, 0.2795, 0.0141, 0.6063, 0.0000, 2.4026, 0.2036, 0.2165, 2.2787,\n",
      "         0.6154, 1.9223, 0.0000, 0.2723, 0.1783],\n",
      "        [2.7010, 0.2735, 0.2204, 1.8796, 2.5171, 0.1221, 0.8104, 0.0000, 0.1626,\n",
      "         0.5901, 0.0000, 0.1388, 2.0973, 0.0000, 0.0000, 0.0419, 4.7906, 2.4931,\n",
      "         3.5899, 1.1057, 0.6108, 0.1689, 1.1567, 0.4394, 3.5487, 0.9514, 0.0000,\n",
      "         4.6221, 2.8608, 0.5683, 2.5802, 4.3978],\n",
      "        [0.0000, 3.1051, 3.2633, 0.8296, 1.0485, 0.0000, 0.0000, 2.3113, 0.0086,\n",
      "         0.0000, 3.3257, 0.0000, 0.0000, 0.0000, 0.5425, 2.3513, 4.7178, 0.3850,\n",
      "         2.3143, 1.7799, 0.0000, 2.7366, 1.5190, 0.6130, 3.1038, 2.0429, 0.0000,\n",
      "         6.7023, 4.2146, 0.0000, 1.7716, 1.8759]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "### original ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95) (.,  0.99) ( I,  0.19) ( am,  0.88) ( such,  0.11) ( a,  0.96) ( happy,  0.89) ( man,  0.56) (.,  0.89) ( I,  0.28) ( enjoy,  0.21) ( eating,  0.31) ( oranges,  0.36) (.,  0.85) ( I,  0.15) ( strong,  0.53) (ly,  0.55) ( dis,  0.06) (l,  0.04) (ike,  0.16)\n",
      "### dangerous prefix\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95)\n",
      "> \u001b[0;32m/home/ubuntu/Documents/infembed/models/_core/cb_decoder_llm.py\u001b[0m(170)\u001b[0;36mfull_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    168 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    169 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 170 \u001b[0;31m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcept_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    171 \u001b[0;31m        concept_embeddings = concept_embeddings.reshape(\n",
      "\u001b[0m\u001b[0;32m    172 \u001b[0;31m            \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "False\n",
      "\n",
      "tensor([[0.0000, 1.0915, 2.5536, 0.0000, 1.3787, 1.7557, 0.0000, 0.1311, 0.1425,\n",
      "         3.8596, 3.6708, 2.5457, 2.0919, 0.0000, 0.6566, 0.0000, 2.3078, 0.0000,\n",
      "         3.7731, 0.0000, 0.0000, 2.7002, 0.0000, 0.4097, 1.4662, 3.5177, 0.0000,\n",
      "         5.2225, 4.0944, 1.9492, 3.6672, 0.0656],\n",
      "        [2.5551, 0.6981, 1.5837, 3.4094, 0.7616, 0.4128, 0.0000, 1.5723, 0.0000,\n",
      "         0.4449, 1.5017, 0.0000, 3.5399, 0.0000, 0.0000, 4.1010, 1.5090, 0.0000,\n",
      "         4.6298, 3.4191, 0.2864, 0.0000, 0.0000, 0.0000, 4.1387, 4.4021, 0.0000,\n",
      "         4.4195, 4.2901, 0.0000, 1.2325, 0.3081],\n",
      "        [2.7010, 0.2735, 0.2204, 1.8796, 2.5171, 0.1221, 0.8104, 0.0000, 0.1626,\n",
      "         0.5901, 0.0000, 0.1388, 2.0973, 0.0000, 0.0000, 0.0419, 4.7906, 2.4931,\n",
      "         3.5899, 1.1057, 0.6108, 0.1689, 1.1567, 0.4394, 3.5487, 0.9514, 0.0000,\n",
      "         4.6221, 2.8608, 0.5683, 2.5802, 4.3978],\n",
      "        [0.0000, 3.1051, 3.2633, 0.8296, 1.0485, 0.0000, 0.0000, 2.3113, 0.0086,\n",
      "         0.0000, 3.3257, 0.0000, 0.0000, 0.0000, 0.5425, 2.3513, 4.7178, 0.3850,\n",
      "         2.3143, 1.7799, 0.0000, 2.7366, 1.5190, 0.6130, 3.1038, 2.0429, 0.0000,\n",
      "         6.7023, 4.2146, 0.0000, 1.7716, 1.8759]], grad_fn=<SelectBackward0>)\n",
      "> \u001b[0;32m/home/ubuntu/Documents/infembed/models/_core/cb_decoder_llm.py\u001b[0m(171)\u001b[0;36mfull_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    169 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    170 \u001b[0;31m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcept_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 171 \u001b[0;31m        concept_embeddings = concept_embeddings.reshape(\n",
      "\u001b[0m\u001b[0;32m    172 \u001b[0;31m            \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    173 \u001b[0;31m        )  # batch size X sequence length X (number concepts X concept embedding dim)\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/ubuntu/Documents/infembed/models/_core/cb_decoder_llm.py\u001b[0m(172)\u001b[0;36mfull_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    170 \u001b[0;31m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcept_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    171 \u001b[0;31m        concept_embeddings = concept_embeddings.reshape(\n",
      "\u001b[0m\u001b[0;32m--> 172 \u001b[0;31m            \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    173 \u001b[0;31m        )  # batch size X sequence length X (number concepts X concept embedding dim)\n",
      "\u001b[0m\u001b[0;32m    174 \u001b[0;31m        prediction_logits = self.generator(\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/ubuntu/Documents/infembed/models/_core/cb_decoder_llm.py\u001b[0m(171)\u001b[0;36mfull_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    169 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    170 \u001b[0;31m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcept_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 171 \u001b[0;31m        concept_embeddings = concept_embeddings.reshape(\n",
      "\u001b[0m\u001b[0;32m    172 \u001b[0;31m            \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    173 \u001b[0;31m        )  # batch size X sequence length X (number concepts X concept embedding dim)\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/ubuntu/Documents/infembed/models/_core/cb_decoder_llm.py\u001b[0m(174)\u001b[0;36mfull_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    172 \u001b[0;31m            \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    173 \u001b[0;31m        )  # batch size X sequence length X (number concepts X concept embedding dim)\n",
      "\u001b[0m\u001b[0;32m--> 174 \u001b[0;31m        prediction_logits = self.generator(\n",
      "\u001b[0m\u001b[0;32m    175 \u001b[0;31m            \u001b[0mconcept_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    176 \u001b[0;31m        )  # batch size X sequence length X vocab size\n",
      "\u001b[0m\n",
      "torch.Size([1, 26, 128])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1\n",
    "\n",
    "\n",
    "def get_batch_input_ids(batch):\n",
    "    # returns list of tensors\n",
    "    return [\n",
    "        torch.Tensor([id for (id, mask) in zip(ids, attention_mask) if mask == 1]).long()\n",
    "        for (ids, attention_mask) in zip(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "for batch, _ in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    __concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "    input_ids = get_batch_input_ids(batch)\n",
    "    for _example_tokens, _concept_logits, _input_ids in zip(\n",
    "        example_tokens, __concept_logits, input_ids\n",
    "    ):\n",
    "        decider_pos = decider(_example_tokens, _concept_logits)\n",
    "        print(\"### original ###\")\n",
    "        plot([_example_tokens], [_concept_logits])\n",
    "        print(\"### dangerous prefix\")\n",
    "        plot([_example_tokens[: decider_pos + 1]], [_concept_logits[: decider_pos + 1]])\n",
    "        # generate starting with the last position in the prefix\n",
    "        _output_ids = decoder(model, eos_token=1, input_ids=_input_ids[:decoder_pos + 1], temperature=0.5)\n",
    "        # get new ids\n",
    "        _new_ids = torch.cat([_input_ids[:-1], _output_ids])\n",
    "        _new_attention_mask = torch.ones(len(_new_ids))\n",
    "        # get corresponding tokens (batch format)\n",
    "        new_batch = {\n",
    "            \"input_ids\": _new_ids.unsqueeze(0),\n",
    "            \"attention_mask\": _new_attention_mask.unsqueeze(0),\n",
    "        }\n",
    "        new_example_tokens = get_batch_tokens(new_batch)\n",
    "        # feed through model to get concept predictions\n",
    "        new_concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "        print(\"### new ###\")\n",
    "        plot(new_example_tokens, new_concept_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
