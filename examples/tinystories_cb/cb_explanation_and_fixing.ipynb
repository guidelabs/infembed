{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the cb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBDecoderLightningModule(\n",
       "  (decoder): CBDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (attention): MultiAttention(\n",
       "          (attentions): ModuleList(\n",
       "            (0-15): 16 x Attention(\n",
       "              (key): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (query): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=48, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attention_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "        (feedforward_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator): MLP(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=10000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (embedder): Sequential(\n",
       "      (0): Embedding(10000, 768)\n",
       "      (1): PositionEncoder(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (positive_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (negative_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (concept_generator): LLMBinaryMultitaskMLPGenerator(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "from models._core.cb_decoder_llm import CBDecoderLightningModule, constructor\n",
    "from models._utils.common import default_checkpoints_load_func, load_model\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "    model=CBDecoderLightningModule(\n",
    "        decoder=constructor(\n",
    "            model_dim=768,\n",
    "            key_dim=48,\n",
    "            value_dim=48,\n",
    "            num_heads=16,\n",
    "            num_layers=4,\n",
    "            dropout=0.0,\n",
    "            hidden_dim=3072,\n",
    "            num_tokens=10000,\n",
    "            max_len=2048,\n",
    "            num_concepts=4,\n",
    "            concept_embedding_dim=32,\n",
    "            concept_embedder_hidden_dims=None,\n",
    "            concept_generator_hidden_dims=None,\n",
    "            generator_hidden_dims=None,\n",
    "        )\n",
    "    ),\n",
    "    eval=True,\n",
    "    checkpoints_load_func=functools.partial(default_checkpoints_load_func, key='state_dict'),\n",
    "    checkpoint=\"/home/ubuntu/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/cb_simplified_read_julius_only_accum_2/lightning_logs/fqtzvlgh/checkpoints/epoch=5-step=1386.ckpt\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the dataloader to get explanations and guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[  41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,  376,\n",
       "           500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,   76,\n",
       "          1322, 5743]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]]),\n",
       " 'input_ids': tensor([[   0,   41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,\n",
       "           376,  500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,\n",
       "            76, 1322]]),\n",
       " 'mask': tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data._core.tinystories import tinystories_tokenizer\n",
    "from data._utils.llm import DecoderLLMCollateFn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_path = '/home/ubuntu/Documents/infembed/files/tinystories/generations/wandb_export_2024-02-22T12_01_34.815-05_00.csv'\n",
    "\n",
    "# define adhoc dataset\n",
    "class GenerationDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.df[\"generation\"].iloc[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "dataset = GenerationDataset(dataset_path)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=DecoderLLMCollateFn(\n",
    "        tokenizer=tinystories_tokenizer(),\n",
    "        max_len=512,\n",
    "    ),\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to get tokens for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data._core.tinystories import tinystories_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = tinystories_tokenizer()\n",
    "\n",
    "def get_batch_tokens(batch):\n",
    "    example_tokens = []\n",
    "    for _input_ids, _attention_mask in zip(\n",
    "        batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "    ):\n",
    "        _example_tokens = [\n",
    "            tokenizer.decode(id)\n",
    "            for (id, mask) in zip(_input_ids, _attention_mask)\n",
    "            if mask == 1\n",
    "        ]\n",
    "        example_tokens.append(_example_tokens)\n",
    "\n",
    "    return example_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to get concept predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_batch_predictions(batch):\n",
    "    raw_concept_logits = model.forward(batch)[\"concept_logits\"]\n",
    "    concept_logits = []\n",
    "    for c in range(raw_concept_logits.shape[2]):\n",
    "        _concept_logits = raw_concept_logits[:, :, c]\n",
    "        __concept_logits = [\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    p\n",
    "                    for (p, mask) in zip(__prediction_logits, _attention_mask)\n",
    "                    if mask == 1\n",
    "                ]\n",
    "            )\n",
    "            for (__prediction_logits, _attention_mask) in zip(\n",
    "                _concept_logits, batch[\"attention_mask\"]\n",
    "            )\n",
    "        ]\n",
    "        concept_logits.append(__concept_logits)\n",
    "    return concept_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to plot tokens and concept predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def plot(example_tokens, concept_logits):\n",
    "    for _example_tokens, _concept_logits in zip(\n",
    "        example_tokens, concept_logits\n",
    "    ):\n",
    "        print(\n",
    "            \" \".join(\n",
    "                [\n",
    "                    f\"({token}, {torch.sigmoid(logit): .2f})\"\n",
    "                    for (token, logit) in zip(_example_tokens, _concept_logits)\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### task 1 ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95) (.,  0.99) ( I,  0.19) ( am,  0.88) ( such,  0.11) ( a,  0.96) ( happy,  0.89) ( man,  0.56) (.,  0.89) ( I,  0.28) ( enjoy,  0.21) ( eating,  0.31) ( oranges,  0.36) (.,  0.85) ( I,  0.15) ( strong,  0.53) (ly,  0.55) ( dis,  0.06) (l,  0.04) (ike,  0.16)\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1\n",
    "concepts = [\n",
    "    # 0,\n",
    "    1,\n",
    "    # 2,\n",
    "    # 3,\n",
    "]\n",
    "\n",
    "for batch, _ in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    concept_logits = get_batch_predictions(batch)\n",
    "    for c in concepts:\n",
    "        print(f\"\\n ### task {c} ###\")\n",
    "        plot(example_tokens, concept_logits[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all analysis below assumes a particular concept we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_concept = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to decide when to start generating on the basis of tokens and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decider(threshold, _example_tokens, _concept_logits):\n",
    "    # `_concept_logits` is for a particular concept\n",
    "    return (_concept_logits > threshold).float().argmax()\n",
    "\n",
    "decider = functools.partial(_decider, 0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to regenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models._utils.cb_llm import ConstantStrategy, GreedyCBDecoder\n",
    "\n",
    "\n",
    "decoder = GreedyCBDecoder(max_len=512, strategy=ConstantStrategy([-1, 0.0, -1, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each example, shorten using decider, print original, shortened, new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### original ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95) (.,  0.99) ( I,  0.19) ( am,  0.88) ( such,  0.11) ( a,  0.96) ( happy,  0.89) ( man,  0.56) (.,  0.89) ( I,  0.28) ( enjoy,  0.21) ( eating,  0.31) ( oranges,  0.36) (.,  0.85) ( I,  0.15) ( strong,  0.53) (ly,  0.55) ( dis,  0.06) (l,  0.04) (ike,  0.16)\n",
      "### dangerous prefix\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95)\n",
      "> \u001b[0;32m/tmp/ipykernel_2951433/2156312730.py\u001b[0m(27)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0;31m# generate starting with the last position in the prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m        \u001b[0m_output_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdecider_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;31m# get new ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m        \u001b[0m_new_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'I', ' am', ' such', ' a', ' happy', ' girl', '.', ' I', ' am', ' such', ' a', ' happy', ' man', '.', ' I', ' enjoy', ' eating', ' oranges', '.', ' I', ' strong', 'ly', ' dis', 'l', 'ike']\n",
      "26\n",
      "\n",
      "tensor(6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1\n",
    "\n",
    "\n",
    "def get_batch_input_ids(batch):\n",
    "    # returns list of tensors\n",
    "    return [\n",
    "        torch.Tensor([id for (id, mask) in zip(ids, attention_mask) if mask == 1]).long()\n",
    "        for (ids, attention_mask) in zip(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "for batch, _ in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    __concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "    input_ids = get_batch_input_ids(batch)\n",
    "    for _example_tokens, _concept_logits, _input_ids in zip(\n",
    "        example_tokens, __concept_logits, input_ids\n",
    "    ):\n",
    "        decider_pos = decider(_example_tokens, _concept_logits)\n",
    "        print(\"### original ###\")\n",
    "        plot([_example_tokens], [_concept_logits])\n",
    "        print(\"### dangerous prefix\")\n",
    "        plot([_example_tokens[: decider_pos + 1]], [_concept_logits[: decider_pos + 1]])\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        # generate starting with the last position in the prefix\n",
    "        _output_ids = decoder(model, eos_token=1, input_ids=_input_ids[:decider_pos + 1], temperature=0.5)\n",
    "        # get new ids\n",
    "        _new_ids = torch.cat([_input_ids[:-1], _output_ids])\n",
    "        _new_attention_mask = torch.ones(len(_new_ids))\n",
    "        # get corresponding tokens (batch format)\n",
    "        new_batch = {\n",
    "            \"input_ids\": _new_ids.unsqueeze(0),\n",
    "            \"attention_mask\": _new_attention_mask.unsqueeze(0),\n",
    "        }\n",
    "        new_example_tokens = get_batch_tokens(new_batch)\n",
    "        # feed through model to get concept predictions\n",
    "        new_concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "        print(\"### new ###\")\n",
    "        plot(new_example_tokens, new_concept_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
