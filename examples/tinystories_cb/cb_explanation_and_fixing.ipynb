{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/fultonwang/Documents/infembed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the cb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBDecoderLightningModule(\n",
       "  (decoder): CBDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (attention): MultiAttention(\n",
       "          (attentions): ModuleList(\n",
       "            (0-15): 16 x Attention(\n",
       "              (key): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (query): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=48, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attention_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "        (feedforward_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator): MLP(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=10000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (embedder): Sequential(\n",
       "      (0): Embedding(10000, 768)\n",
       "      (1): PositionEncoder(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (positive_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (negative_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (concept_generator): LLMBinaryMultitaskMLPGenerator(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "from models._core.cb_decoder_llm import CBDecoderLightningModule, constructor\n",
    "from models._utils.common import default_checkpoints_load_func, load_model\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "    model=CBDecoderLightningModule(\n",
    "        decoder=constructor(\n",
    "            model_dim=768,\n",
    "            key_dim=48,\n",
    "            value_dim=48,\n",
    "            num_heads=16,\n",
    "            num_layers=4,\n",
    "            dropout=0.0,\n",
    "            hidden_dim=3072,\n",
    "            num_tokens=10000,\n",
    "            max_len=2048,\n",
    "            num_concepts=4,\n",
    "            concept_embedding_dim=32,\n",
    "            concept_embedder_hidden_dims=None,\n",
    "            concept_generator_hidden_dims=None,\n",
    "            generator_hidden_dims=None,\n",
    "        )\n",
    "    ),\n",
    "    eval=True,\n",
    "    checkpoints_load_func=functools.partial(default_checkpoints_load_func, key='state_dict'),\n",
    "    checkpoint=\"/Users/fultonwang/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/cb_simplified_read_julius_only_accum_2/lightning_logs/fqtzvlgh/checkpoints/epoch=5-step=1386.ckpt\",\n",
    "    # checkpoint=\"/home/ubuntu/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/cb_simplified_read_julius_only_accum_2/lightning_logs/fqtzvlgh/checkpoints/epoch=5-step=1386.ckpt\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the dataloader to get explanations and guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[  41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,  376,\n",
       "           500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,   76,\n",
       "          1322, 5743]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]]),\n",
       " 'input_ids': tensor([[   0,   41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,\n",
       "           376,  500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,\n",
       "            76, 1322]]),\n",
       " 'mask': tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data._core.tinystories import tinystories_tokenizer\n",
    "from data._utils.llm import DecoderLLMCollateFn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_path = '/Users/fultonwang/Documents/infembed/files/tinystories/generations/wandb_export_2024-02-22T12_01_34.815-05_00.csv'\n",
    "# dataset_path = '/home/ubuntu/Documents/infembed/files/tinystories/generations/wandb_export_2024-02-22T12_01_34.815-05_00.csv'\n",
    "\n",
    "# define adhoc dataset\n",
    "class GenerationDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.df[\"generation\"].iloc[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "dataset = GenerationDataset(dataset_path)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=DecoderLLMCollateFn(\n",
    "        tokenizer=tinystories_tokenizer(),\n",
    "        max_len=512,\n",
    "    ),\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to get tokens for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data._core.tinystories import tinystories_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = tinystories_tokenizer()\n",
    "\n",
    "def get_batch_tokens(batch):\n",
    "    example_tokens = []\n",
    "    for _input_ids, _attention_mask in zip(\n",
    "        batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "    ):\n",
    "        _example_tokens = [\n",
    "            tokenizer.decode(id)\n",
    "            for (id, mask) in zip(_input_ids, _attention_mask)\n",
    "            if mask == 1\n",
    "        ]\n",
    "        example_tokens.append(_example_tokens)\n",
    "\n",
    "    return example_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to get concept predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_batch_predictions(batch):\n",
    "    raw_concept_logits = model.forward(batch)[\"concept_logits\"]\n",
    "    concept_logits = []\n",
    "    for c in range(raw_concept_logits.shape[2]):\n",
    "        _concept_logits = raw_concept_logits[:, :, c]\n",
    "        __concept_logits = [\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    p\n",
    "                    for (p, mask) in zip(__prediction_logits, _attention_mask)\n",
    "                    if mask == 1\n",
    "                ]\n",
    "            )\n",
    "            for (__prediction_logits, _attention_mask) in zip(\n",
    "                _concept_logits, batch[\"attention_mask\"]\n",
    "            )\n",
    "        ]\n",
    "        concept_logits.append(__concept_logits)\n",
    "    return concept_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to plot tokens and concept predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def plot(example_tokens, concept_logits):\n",
    "    for _example_tokens, _concept_logits in zip(\n",
    "        example_tokens, concept_logits\n",
    "    ):\n",
    "        print(\n",
    "            \" \".join(\n",
    "                [\n",
    "                    f\"({token}, {torch.sigmoid(logit): .2f})\"\n",
    "                    for (token, logit) in zip(_example_tokens, _concept_logits)\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### task 1 ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95) (.,  0.99) ( I,  0.19) ( am,  0.88) ( such,  0.11) ( a,  0.96) ( happy,  0.89) ( man,  0.56) (.,  0.89) ( I,  0.28) ( enjoy,  0.21) ( eating,  0.31) ( oranges,  0.36) (.,  0.85) ( I,  0.15) ( strong,  0.53) (ly,  0.55) ( dis,  0.06) (l,  0.04) (ike,  0.16)\n",
      "\n",
      " ### task 2 ###\n",
      "(<|endoftext|>,  0.82) (I,  0.00) ( am,  0.00) ( such,  0.06) ( a,  0.08) ( happy,  0.16) ( girl,  0.08) (.,  0.04) ( I,  0.32) ( am,  0.66) ( such,  0.28) ( a,  0.80) ( happy,  0.79) ( man,  0.99) (.,  0.95) ( I,  0.86) ( enjoy,  0.74) ( eating,  0.29) ( oranges,  0.56) (.,  0.90) ( I,  0.63) ( strong,  0.96) (ly,  0.72) ( dis,  0.23) (l,  0.10) (ike,  0.31)\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1\n",
    "concepts = [\n",
    "    # 0,\n",
    "    1,\n",
    "    2,\n",
    "    # 3,\n",
    "]\n",
    "\n",
    "for batch, _ in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    concept_logits = get_batch_predictions(batch)\n",
    "    for c in concepts:\n",
    "        print(f\"\\n ### task {c} ###\")\n",
    "        plot(example_tokens, concept_logits[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all analysis below assumes a particular concept we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_concept = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to decide when to start generating on the basis of tokens and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decider(threshold, _example_tokens, _concept_logits):\n",
    "    # `_concept_logits` is for a particular concept\n",
    "    return (_concept_logits > threshold).float().argmax()\n",
    "\n",
    "decider = functools.partial(_decider, 0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to regenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models._utils.cb_llm import ConstantStrategy, GreedyCBDecoder\n",
    "\n",
    "\n",
    "decoder = GreedyCBDecoder(max_len=512, strategy=ConstantStrategy([-1, 0.0, 1.0, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each example, shorten using decider, print original, shortened, new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### original ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95) (.,  0.99) ( I,  0.19) ( am,  0.88) ( such,  0.11) ( a,  0.96) ( happy,  0.89) ( man,  0.56) (.,  0.89) ( I,  0.28) ( enjoy,  0.21) ( eating,  0.31) ( oranges,  0.36) (.,  0.85) ( I,  0.15) ( strong,  0.53) (ly,  0.55) ( dis,  0.06) (l,  0.04) (ike,  0.16)\n",
      "### dangerous prefix\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.00) ( such,  0.01) ( a,  0.07) ( happy,  0.35) ( girl,  0.95)\n",
      "> \u001b[0;32m/Users/fultonwang/Documents/infembed/models/_utils/cb_llm.py\u001b[0m(91)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     89 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     90 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 91 \u001b[0;31m            \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# get logits in last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     92 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     93 \u001b[0;31m                \u001b[0mtop_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/Users/fultonwang/Documents/infembed/models/_utils/cb_llm.py\u001b[0m(92)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     90 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     91 \u001b[0;31m            \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# get logits in last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 92 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     93 \u001b[0;31m                \u001b[0mtop_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor(14)\n",
      "(tensor([ -6.5605, -14.9600, -15.9398,  ..., -85.1517, -85.2563, -87.0616],\n",
      "       grad_fn=<SortBackward0>), tensor([  14,  658,   12,  ..., 8078, 6474, 7543]))\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1\n",
    "\n",
    "\n",
    "def get_batch_input_ids(batch):\n",
    "    # returns list of tensors\n",
    "    return [\n",
    "        torch.Tensor([id for (id, mask) in zip(ids, attention_mask) if mask == 1]).long()\n",
    "        for (ids, attention_mask) in zip(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "for batch, _ in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    __concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "    input_ids = get_batch_input_ids(batch)\n",
    "    for _example_tokens, _concept_logits, _input_ids in zip(\n",
    "        example_tokens, __concept_logits, input_ids\n",
    "    ):\n",
    "        decider_pos = decider(_example_tokens, _concept_logits)\n",
    "        print(\"### original ###\")\n",
    "        plot([_example_tokens], [_concept_logits])\n",
    "        print(\"### dangerous prefix\")\n",
    "        plot([_example_tokens[: decider_pos + 1]], [_concept_logits[: decider_pos + 1]])\n",
    "        # generate starting with the last position in the prefix\n",
    "        _output_ids = decoder(model, eos_token=1, input_ids=_input_ids[:decider_pos + 1], temperature=0.5)\n",
    "        # get new ids\n",
    "        _new_ids = torch.cat([_input_ids[:-1], _output_ids])\n",
    "        _new_attention_mask = torch.ones(len(_new_ids))\n",
    "        # get corresponding tokens (batch format)\n",
    "        new_batch = {\n",
    "            \"input_ids\": _new_ids.unsqueeze(0),\n",
    "            \"attention_mask\": _new_attention_mask.unsqueeze(0),\n",
    "        }\n",
    "        new_example_tokens = get_batch_tokens(new_batch)\n",
    "        # feed through model to get concept predictions\n",
    "        new_concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "        print(\"### new ###\")\n",
    "        plot(new_example_tokens, new_concept_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_kernel",
   "language": "python",
   "name": "test_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
