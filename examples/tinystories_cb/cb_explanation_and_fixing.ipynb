{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/fultonwang/Documents/infembed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the cb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fultonwang/anaconda3/envs/test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBDecoderLightningModule(\n",
       "  (decoder): CBDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (attention): MultiAttention(\n",
       "          (attentions): ModuleList(\n",
       "            (0-15): 16 x Attention(\n",
       "              (key): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (query): Linear(in_features=768, out_features=48, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=48, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attention_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "        (feedforward_sublayer): Sublayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator): MLP(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=10000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (embedder): Sequential(\n",
       "      (0): Embedding(10000, 768)\n",
       "      (1): PositionEncoder(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (positive_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (negative_concept_embedder): MLPConceptEmbedder(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (concept_generator): LLMBinaryMultitaskMLPGenerator(\n",
       "      (mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "from models._core.cb_decoder_llm import CBDecoderLightningModule, constructor\n",
    "from models._utils.common import default_checkpoints_load_func, load_model\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "    model=CBDecoderLightningModule(\n",
    "        decoder=constructor(\n",
    "            model_dim=768,\n",
    "            key_dim=48,\n",
    "            value_dim=48,\n",
    "            num_heads=16,\n",
    "            num_layers=4,\n",
    "            dropout=0.0,\n",
    "            hidden_dim=3072,\n",
    "            num_tokens=10000,\n",
    "            max_len=2048,\n",
    "            num_concepts=4,\n",
    "            concept_embedding_dim=32,\n",
    "            concept_embedder_hidden_dims=None,\n",
    "            concept_generator_hidden_dims=None,\n",
    "            generator_hidden_dims=None,\n",
    "        )\n",
    "    ),\n",
    "    eval=True,\n",
    "    checkpoints_load_func=functools.partial(default_checkpoints_load_func, key='state_dict'),\n",
    "    # checkpoint=\"/Users/fultonwang/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/cb_simplified_read_julius_only_accum_2/lightning_logs/fqtzvlgh/checkpoints/epoch=5-step=1386.ckpt\",\n",
    "    checkpoint=\"/Users/fultonwang/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/cb_simplified_read_julius_only_accum_2/lightning_logs/fqtzvlgh/checkpoints/epoch=3-step=924.ckpt\",\n",
    "    # checkpoint=\"/home/ubuntu/Documents/infembed/examples/tinystories_cb/hydra_outputs/lightning_train/cb_simplified_read_julius_only_accum_2/lightning_logs/fqtzvlgh/checkpoints/epoch=5-step=1386.ckpt\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the dataloader to get explanations and guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[  41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,  376,\n",
       "           500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,   76,\n",
       "          1322, 5743]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]]),\n",
       " 'input_ids': tensor([[   0,   41,  740, 2402,  259,  376,  477,   14,  338,  740, 2402,  259,\n",
       "           376,  500,   14,  338, 1218, 1735, 5190,   14,  338,  964,  460, 1233,\n",
       "            76, 1322]]),\n",
       " 'mask': tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data._core.tinystories import tinystories_tokenizer\n",
    "from data._utils.llm import DecoderLLMCollateFn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_path = '/Users/fultonwang/Documents/infembed/files/tinystories/generations/wandb_export_2024-02-22T12_01_34.815-05_00.csv'\n",
    "# dataset_path = '/home/ubuntu/Documents/infembed/files/tinystories/generations/wandb_export_2024-02-22T12_01_34.815-05_00.csv'\n",
    "\n",
    "# define adhoc dataset\n",
    "class GenerationDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.df[\"generation\"].iloc[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "dataset = GenerationDataset(dataset_path)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=DecoderLLMCollateFn(\n",
    "        tokenizer=tinystories_tokenizer(),\n",
    "        max_len=512,\n",
    "    ),\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/tpn62f1149951598g16hpts00000gn/T/ipykernel_13704/3039558482.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display\n"
     ]
    }
   ],
   "source": [
    "from data._core.tinystories import tinystories_tokenizer\n",
    "from models._utils.binary_multitask_llm import _last_token_multitask_get_preds\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "\n",
    "tokenizer = tinystories_tokenizer()\n",
    "\n",
    "\n",
    "def get_batch_tokens(input_ids, attention_mask):\n",
    "    example_tokens = []\n",
    "    for _input_ids, _attention_mask in zip(input_ids, attention_mask):\n",
    "        _example_tokens = [\n",
    "            tokenizer.decode(id)\n",
    "            for (id, mask) in zip(_input_ids, _attention_mask)\n",
    "            if mask == 1\n",
    "        ]\n",
    "        example_tokens.append(_example_tokens)\n",
    "\n",
    "    return example_tokens\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_batch_predictions(input_ids, attention_mask, mask):\n",
    "    raw_concept_logits = model.forward({\"input_ids\": input_ids, \"mask\": mask})[\n",
    "        \"concept_logits\"\n",
    "    ]\n",
    "    concept_logits = []\n",
    "    for _concept_logits, _attention_mask in zip(raw_concept_logits, attention_mask):\n",
    "        concept_logits.append(\n",
    "            torch.stack([__concept_logits for (__concept_logits, mask) in zip(_concept_logits, _attention_mask) if mask == 1], dim=0)\n",
    "        )\n",
    "    return concept_logits\n",
    "\n",
    "\n",
    "def plot_example_tokens_single_concept(_example_tokens, _concept_logits, c):\n",
    "    \"\"\"\n",
    "    plots example, for the provided concept\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \" \".join(\n",
    "            [\n",
    "                f\"({token}, {torch.sigmoid(logit): .2f})\"\n",
    "                for (token, logit) in zip(_example_tokens, _concept_logits[:, c])\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _plot_example_tokens_single_concept(_example_tokens, _concept_logits, c):\n",
    "    attrs = (torch.sigmoid(_concept_logits[:, c]) - 0.5) * 2\n",
    "    # attrs = torch.sigmoid(_concept_logits[:, c])\n",
    "    # rgb = lambda x: '255,0,0' if x < 0 else '0,255,0'\n",
    "    rgb = lambda x: '0,255,0' if x < 0 else '255,0,0'\n",
    "    # rgb = lambda x: '255,0,0'\n",
    "    alpha = lambda x: abs(x) ** 0.25\n",
    "    token_marks = [\n",
    "        f'<mark style=\"background-color:rgba({rgb(attr)},{alpha(attr)})\">{token}</mark>'\n",
    "        for token, attr in zip(_example_tokens[1:], attrs.tolist()[1:])\n",
    "    ]\n",
    "    \n",
    "    display(HTML('<p>' + ' '.join(token_marks) + '</p>'))\n",
    "\n",
    "\n",
    "def get_batch_example_predictions(concept_logits):\n",
    "    return [_concept_logits[-1].detach().numpy() for _concept_logits in concept_logits]\n",
    "\n",
    "\n",
    "def plot_example_all_concepts(_example_tokens, _example_concept_logits):\n",
    "    print(''.join(_example_tokens[1:]))\n",
    "    fig = plt.figure(figsize=(4,3))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    # fig, ax = plt.subplots()\n",
    "    fig.suptitle('probability of different concepts', fontsize=20)\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.set_ylim((0,1))\n",
    "    pd.Series(dict(zip(range(_example_concept_logits.shape[1]), torch.sigmoid(torch.Tensor(_example_concept_logits)).numpy()))).plot.bar(ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "def print_tokens(_tokens):\n",
    "    print(''.join(_tokens))\n",
    "\n",
    "def print_ids(_ids):\n",
    "    print_tokens(tokenizer.decode(_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot per-token concept predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### concept 1 ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.01) ( such,  0.00) ( a,  0.06) ( happy,  0.33) ( girl,  0.92) (.,  0.97) ( I,  0.16) ( am,  0.85) ( such,  0.16) ( a,  0.93) ( happy,  0.91) ( man,  0.50) (.,  0.90) ( I,  0.32) ( enjoy,  0.30) ( eating,  0.30) ( oranges,  0.39) (.,  0.91) ( I,  0.23) ( strong,  0.57) (ly,  0.48) ( dis,  0.06) (l,  0.08) (ike,  0.11)\n",
      "\n",
      " ### concept 2 ###\n",
      "(<|endoftext|>,  0.84) (I,  0.00) ( am,  0.00) ( such,  0.05) ( a,  0.07) ( happy,  0.18) ( girl,  0.05) (.,  0.04) ( I,  0.20) ( am,  0.36) ( such,  0.25) ( a,  0.76) ( happy,  0.75) ( man,  0.99) (.,  0.91) ( I,  0.84) ( enjoy,  0.73) ( eating,  0.30) ( oranges,  0.54) (.,  0.80) ( I,  0.63) ( strong,  0.94) (ly,  0.76) ( dis,  0.20) (l,  0.24) (ike,  0.32)\n",
      "\n",
      " ### concept 1 ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.01) ( such,  0.00) ( a,  0.06) ( happy,  0.33) ( girl,  0.92) (.,  0.97) ( I,  0.16) ( am,  0.85) ( such,  0.16) ( a,  0.93) ( happy,  0.91) ( man,  0.50) (.,  0.90) ( I,  0.32) ( enjoy,  0.30) ( eating,  0.30) ( oranges,  0.39) (.,  0.91) ( I,  0.23) ( strong,  0.57) (ly,  0.48) ( dis,  0.06) (l,  0.08) (ike,  0.11)\n",
      "\n",
      " ### concept 2 ###\n",
      "(<|endoftext|>,  0.84) (I,  0.00) ( am,  0.00) ( such,  0.05) ( a,  0.07) ( happy,  0.18) ( girl,  0.05) (.,  0.04) ( I,  0.20) ( am,  0.36) ( such,  0.25) ( a,  0.76) ( happy,  0.75) ( man,  0.99) (.,  0.91) ( I,  0.84) ( enjoy,  0.73) ( eating,  0.30) ( oranges,  0.54) (.,  0.80) ( I,  0.63) ( strong,  0.94) (ly,  0.76) ( dis,  0.20) (l,  0.24) (ike,  0.32)\n"
     ]
    }
   ],
   "source": [
    "cs = [1,2]\n",
    "\n",
    "for batch, _ in zip(dataloader, range(2)):\n",
    "    example_tokens = get_batch_tokens(batch['input_ids'], batch['attention_mask'])\n",
    "    concept_logits = get_batch_predictions(batch['input_ids'], batch['attention_mask'], batch['mask'])\n",
    "    for c in cs:\n",
    "        print(f\"\\n ### concept {c} ###\")\n",
    "        for _example_tokens, _concept_logits in zip(example_tokens, concept_logits):\n",
    "            plot_example_tokens_single_concept(_example_tokens, _concept_logits, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare one-pass generations to generations if starting with different prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n",
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike poems! Once upon a time there was a little girl named Lily. She was very curious and wanted to explore the world.\n"
     ]
    }
   ],
   "source": [
    "from models._utils.cb_llm import GreedyCBDecoder\n",
    "\n",
    "\n",
    "decoder = GreedyCBDecoder(max_len=512)\n",
    "\n",
    "input_id = torch.Tensor([0]).long()\n",
    "oneshot = decoder(\n",
    "    model,\n",
    "    eos_token=0,\n",
    "    input_ids=torch.Tensor([0]).long(),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "for t in range(10):\n",
    "    head = torch.cat([input_id, oneshot[:t]])\n",
    "    # head = oneshot[:t+1]\n",
    "\n",
    "    tail = decoder(\n",
    "        model,\n",
    "        eos_token=0,\n",
    "        input_ids=head,\n",
    "        # input_ids=input_id,\n",
    "        temperature=0,\n",
    "    )\n",
    "    print(tokenizer.decode(torch.cat([head, tail])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to decide when to start generating on the basis of tokens and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decider(threshold, c, prep_steps, _example_tokens, _concept_logits):\n",
    "    return torch.max((torch.sigmoid(_concept_logits[:, c]) > threshold).float().argmax() - prep_steps, 0).values\n",
    "\n",
    "decider = functools.partial(_decider, 0.90, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each example, regenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>I am such a happy girl. I am such a happy man. I enjoy eating oranges. I strongly dislike\n",
      "<|endoftext|>I am such\n",
      "<|endoftext|>I am such a happy girl. I am such a happy boy. I enjoy eating oranges. I am such a happy girl. I am such a happy girl.\n",
      "> \u001b[0;32m/var/folders/hb/tpn62f1149951598g16hpts00000gn/T/ipykernel_13704/55198528.py\u001b[0m(20)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     16 \u001b[0;31m        \u001b[0mtail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m        \u001b[0mprint_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     18 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 20 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([[ 1.5954,  0.8001,  1.4446,  0.2316],\n",
      "        [-1.1748, -0.6768,  0.5953, -2.2600],\n",
      "        [ 1.6213, -0.1765,  1.6003,  0.8364],\n",
      "        [ 0.0236,  0.3453,  1.6015, -0.7578],\n",
      "        [ 0.9666,  1.9228,  2.3193, -1.8900],\n",
      "        [ 0.4621,  1.4308,  3.7502, -1.0188],\n",
      "        [ 0.2215,  3.8125,  1.7750,  0.8181],\n",
      "        [ 1.4771,  2.6986,  2.4015,  0.5246],\n",
      "        [-1.0237, -0.9713,  0.4538, -2.5780],\n",
      "        [ 1.6344, -0.4428,  1.4758,  1.0269],\n",
      "        [ 0.2281, -0.0662,  1.6684, -0.8741],\n",
      "        [ 0.9089,  2.0660,  2.4254, -1.9085],\n",
      "        [ 0.3944,  1.3596,  3.6943, -0.9464],\n",
      "        [ 0.4431,  0.2564,  4.3566, -0.1562],\n",
      "        [ 1.5164,  2.7123,  2.4782,  0.5790],\n",
      "        [-1.1212, -0.8412,  0.7124, -2.4449],\n",
      "        [ 0.7328, -1.3018,  0.6345,  0.3518],\n",
      "        [-0.2409, -1.2692, -0.5449,  1.5565],\n",
      "        [ 1.0583,  0.7144,  1.1863,  2.5279],\n",
      "        [ 1.4880,  2.6948,  2.3835,  0.5243],\n",
      "        [-1.1330, -0.9893,  0.5133, -2.5441],\n",
      "        [ 0.7926, -0.1350,  2.7147, -0.6809],\n",
      "        [ 2.7878,  0.6351,  1.7777,  0.6130],\n",
      "        [-0.4504, -3.1615, -1.2132, -2.2078],\n",
      "        [-0.0784, -2.2990, -0.8150, -1.2503],\n",
      "        [ 1.4047, -1.5181, -0.6081,  0.6647]], grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from models._utils.cb_llm import ConstantStrategy\n",
    "\n",
    "\n",
    "decoder = GreedyCBDecoder(max_len=512, strategy=ConstantStrategy([-1, 0, -1, -1]))\n",
    "\n",
    "\n",
    "for batch, _ in zip(dataloader, range(2)):\n",
    "    input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "    concept_logits = model.decoder.full_generate(input_ids, attention_mask)['concept_logits']\n",
    "    for (_input_ids, _attention_mask, _concept_logits) in zip(input_ids, attention_mask, concept_logits):\n",
    "        print_ids(_input_ids)\n",
    "        _example_tokens = tokenizer.decode(_input_ids)\n",
    "        decider_pos = decider(_example_tokens, _concept_logits)\n",
    "        head = _input_ids[:decider_pos]\n",
    "        print_ids(head)\n",
    "        tail = decoder(model=model, eos_token=0, input_ids=head)\n",
    "        print_ids(torch.cat([head, tail]))\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how to regenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models._utils.cb_llm import ConstantStrategy, GreedyCBDecoder\n",
    "\n",
    "\n",
    "decoder = GreedyCBDecoder(max_len=512, strategy=ConstantStrategy([-1, 0, -1, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each example, shorten using decider, print original, shortened, new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decider_pos tensor(6)\n",
      "decider token  girl\n",
      "### original ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.01) ( such,  0.00) ( a,  0.06) ( happy,  0.33) ( girl,  0.92) (.,  0.97) ( I,  0.16) ( am,  0.85) ( such,  0.16) ( a,  0.93) ( happy,  0.91) ( man,  0.50) (.,  0.90) ( I,  0.32) ( enjoy,  0.30) ( eating,  0.30) ( oranges,  0.39) (.,  0.91) ( I,  0.23) ( strong,  0.57) (ly,  0.48) ( dis,  0.06) (l,  0.08) (ike,  0.11)\n",
      "### dangerous prefix\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.01) ( such,  0.00) ( a,  0.06) ( happy,  0.33) ( girl,  0.92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/tpn62f1149951598g16hpts00000gn/T/ipykernel_13704/1564019124.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  next_probs = F.softmax(decoder.next_logits(model, _input_ids[:decider_pos + 1], initial_pos=decider_pos))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### new ###\n",
      "(<|endoftext|>,  0.10) (I,  0.01) ( am,  0.01) ( such,  0.00) ( a,  0.06) ( happy,  0.33) (.,  0.17) ( I,  0.03) ( am,  0.17) ( such,  0.03) ( a,  0.33) ( happy,  0.45) ( man,  0.05) (.,  0.18) ( I,  0.11) ( enjoy,  0.15) ( eating,  0.14) ( oranges,  0.18) (.,  0.72) ( I,  0.14) ( strong,  0.23) (ly,  0.22) ( dis,  0.04) (l,  0.03) (ike,  0.05) ( poems,  0.18)\n",
      "\n",
      ".,! who and to in said was at\n",
      "[(tensor(14), '.', tensor(0.9993, grad_fn=<UnbindBackward0>)), (tensor(12), ',', tensor(0.0003, grad_fn=<UnbindBackward0>)), (tensor(1), '!', tensor(0.0002, grad_fn=<UnbindBackward0>)), (tensor(658), ' who', tensor(7.9670e-05, grad_fn=<UnbindBackward0>)), (tensor(267), ' and', tensor(6.9575e-05, grad_fn=<UnbindBackward0>)), (tensor(266), ' to', tensor(9.9911e-06, grad_fn=<UnbindBackward0>)), (tensor(313), ' in', tensor(9.4612e-06, grad_fn=<UnbindBackward0>)), (tensor(324), ' said', tensor(6.6216e-06, grad_fn=<UnbindBackward0>)), (tensor(283), ' was', tensor(5.6884e-06, grad_fn=<UnbindBackward0>)), (tensor(449), ' at', tensor(5.3087e-06, grad_fn=<UnbindBackward0>))]\n",
      "> \u001b[0;32m/var/folders/hb/tpn62f1149951598g16hpts00000gn/T/ipykernel_13704/1564019124.py\u001b[0m(83)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     80 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     81 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     82 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 83 \u001b[0;31m        \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtop_probs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     84 \u001b[0;31m        \u001b[0;31m#print(list(zip(top_tokens, top_probs)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from data._utils.llm import subsequent_mask\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_batches = 1\n",
    "\n",
    "\n",
    "def get_batch_input_ids(batch):\n",
    "    # returns list of tensors\n",
    "    return [\n",
    "        torch.Tensor([id for (id, mask) in zip(ids, attention_mask) if mask == 1]).long()\n",
    "        for (ids, attention_mask) in zip(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "for batch, _ in zip(dataloader, range(num_batches)):\n",
    "    example_tokens = get_batch_tokens(batch)\n",
    "    __concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "    input_ids = get_batch_input_ids(batch)\n",
    "    for _example_tokens, _concept_logits, _input_ids in zip(\n",
    "        example_tokens, __concept_logits, input_ids\n",
    "    ):\n",
    "        decider_pos = decider(_example_tokens, _concept_logits)\n",
    "        print(\"decider_pos\", decider_pos)\n",
    "        print(\"decider token\", tokenizer.decode(_input_ids[decider_pos]))\n",
    "        print(\"### original ###\")\n",
    "        plot([_example_tokens], [_concept_logits])\n",
    "        print(\"### dangerous prefix\")\n",
    "        plot([_example_tokens[: decider_pos + 1]], [_concept_logits[: decider_pos + 1]])\n",
    "        # generate starting with the last position in the prefix\n",
    "        # initial_pos = decider_pos\n",
    "        initial_pos = None\n",
    "        _output_ids = decoder(model, eos_token=1, input_ids=_input_ids[:decider_pos + 1], initial_pos=initial_pos, temperature=None)#temperature=0.5)\n",
    "        # get new ids\n",
    "        # _new_ids = torch.cat([_input_ids[:-1], _output_ids])\n",
    "        _new_ids = torch.cat([_input_ids[:decider_pos + 0], _output_ids])\n",
    "        #_new_ids = _output_ids\n",
    "        _new_attention_mask = torch.ones(len(_new_ids))\n",
    "        # get corresponding tokens (batch format)\n",
    "        new_batch = {\n",
    "            \"input_ids\": _new_ids.unsqueeze(0),\n",
    "            \"attention_mask\": _new_attention_mask.unsqueeze(0),\n",
    "            \"mask\": subsequent_mask(len(_new_ids))\n",
    "        }\n",
    "        new_example_tokens = get_batch_tokens(new_batch)\n",
    "        # feed through model to get concept predictions\n",
    "        new_concept_logits = get_batch_predictions(new_batch)[the_concept]\n",
    "        print(\"### new ###\")\n",
    "        # print(len(_new_ids), len(_input_ids[:-1]), len(_output_ids))\n",
    "        # print(_new_ids)\n",
    "        # print(new_example_tokens)\n",
    "        plot(new_example_tokens, new_concept_logits)\n",
    "\n",
    "        if False:\n",
    "            _new_ids = _output_ids\n",
    "            _new_attention_mask = torch.ones(len(_new_ids))\n",
    "            # get corresponding tokens (batch format)\n",
    "            new_batch = {\n",
    "                \"input_ids\": _new_ids.unsqueeze(0),\n",
    "                \"attention_mask\": _new_attention_mask.unsqueeze(0),\n",
    "            }\n",
    "            new_example_tokens = get_batch_tokens(new_batch)\n",
    "            # feed through model to get concept predictions\n",
    "            new_concept_logits = get_batch_predictions(batch)[the_concept]\n",
    "            print(\"### new ###\")\n",
    "            print(len(_new_ids), len(_output_ids))\n",
    "            print(_new_ids)\n",
    "            print(new_example_tokens)\n",
    "            plot(new_example_tokens, new_concept_logits)\n",
    "\n",
    "        # also look at the top next tokens\n",
    "        print()\n",
    "        import pdb\n",
    "        #pdb.set_trace()\n",
    "        next_probs = F.softmax(decoder.next_logits(model, _input_ids[:decider_pos + 1], initial_pos=decider_pos))\n",
    "        num_to_display = 10\n",
    "        top_ids = next_probs.argsort(descending=True)[:num_to_display]\n",
    "        top_tokens = get_batch_tokens({'input_ids': top_ids.unsqueeze(0), 'attention_mask': torch.ones(len(top_ids)).unsqueeze(0)})[0]\n",
    "        top_probs = next_probs[top_ids]\n",
    "        print(tokenizer.decode(top_ids))\n",
    "        print(list(zip(top_ids, top_tokens, top_probs)))\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        plot([top_tokens], [top_probs])\n",
    "        #print(list(zip(top_tokens, top_probs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_kernel",
   "language": "python",
   "name": "test_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
